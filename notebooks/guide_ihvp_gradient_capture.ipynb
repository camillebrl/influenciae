{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide Complet : Calcul de l'Influence avec Influenciae\n",
    "\n",
    "Ce notebook explique en détail comment les fonctions d'influence sont calculées dans Influenciae, en partant de la définition mathématique jusqu'à l'implémentation.\n",
    "\n",
    "**Ce que vous allez apprendre :**\n",
    "\n",
    "1. **D'où vient la formule d'influence** - Dérivation mathématique complète\n",
    "2. **Pourquoi on utilise Fisher/Gauss-Newton** au lieu de la Hessienne exacte\n",
    "3. **Comment KFAC factorise** la matrice de Fisher par couche\n",
    "4. **Comment EKFAC corrige** les valeurs propres de KFAC\n",
    "5. **Comment Influenciae implémente** ces concepts en TensorFlow\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table des Matières\n",
    "\n",
    "1. [Qu'est-ce que l'influence ?](#1.-Qu'est-ce-que-l'influence-?)\n",
    "2. [Dérivation mathématique de la formule d'influence](#2.-Dérivation-mathématique)\n",
    "3. [Pourquoi Fisher au lieu de la Hessienne ?](#3.-Pourquoi-Fisher-?)\n",
    "4. [KFAC : Factorisation de Kronecker par couche](#4.-KFAC)\n",
    "5. [EKFAC : Correction des valeurs propres](#5.-EKFAC)\n",
    "6. [Implémentation dans Influenciae](#6.-Implémentation)\n",
    "7. [Exemple pratique complet](#7.-Exemple-pratique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy, Reduction\n",
    "\n",
    "# Imports influenciae\n",
    "from deel.influenciae.common import InfluenceModel, FisherIHVP, KFACIHVP, EKFACIHVP\n",
    "from deel.influenciae.common.inverse_hessian_vector_product import ExactIHVP\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Qu'est-ce que l'influence ? <a name=\"1.-Qu'est-ce-que-l'influence-?\"></a>\n",
    "\n",
    "### Le problème : comprendre l'impact des données d'entraînement\n",
    "\n",
    "L'**influence** cherche à approximer le **Leave-one-out**, c'est-à-dire à estimer l'**impact qu'aurait un exemple d'entraînement sur la perte d'un exemple de test**.\n",
    "\n",
    "Les fonctions d'influence permettent de répondre à :\n",
    "- Quels exemples d'entraînement ont été **utiles** à une prédiction ?\n",
    "- Le modèle s'est trompé : sur quels exemples s'est-il **basé** pour cette mauvaise prédiction ?\n",
    "- Est-ce que je devrais **ajouter** cet exemple pour améliorer une prédiction ?\n",
    "\n",
    "### Définition formelle\n",
    "\n",
    "L'influence de $z_{\\rm train}$ sur $z_{\\rm test}$ se définit comme :\n",
    "\n",
    "$$\\mathrm{Influence}(z_{\\rm train}\\to z_{\\rm test}) = \\left.\\frac{d}{d\\varepsilon}\\,\\mathcal{L}\\bigl(z_{\\rm test},\\,\\theta_\\varepsilon(z_{\\text{train}})\\bigr)\\right|_{\\varepsilon=0}$$\n",
    "\n",
    "**Interprétation** : Elle mesure la sensibilité de la loss de $z_{\\rm test}$ à un **\"up-weight\" infinitésimal** de la loss de $z_{\\rm train}$.\n",
    "\n",
    "### Notations\n",
    "\n",
    "| Symbole | Signification |\n",
    "|---------|---------------|\n",
    "| $\\hat{\\theta}$ | Poids optimaux du modèle pré-entraîné |\n",
    "| $\\theta_\\varepsilon(z_{\\text{train}})$ | Poids \"modifiés\" après up-weight de $\\varepsilon$ sur $z_{\\text{train}}$ |\n",
    "| $\\mathcal{L}(z, \\theta)$ | Loss du modèle sur l'exemple $z$ avec les poids $\\theta$ |\n",
    "| $H_\\theta$ | Matrice Hessienne (dérivée seconde par rapport à $\\theta$) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Dérivation mathématique de la formule d'influence <a name=\"2.-Dérivation-mathématique\"></a>\n",
    "\n",
    "On veut calculer :\n",
    "$$\\frac{d}{d\\varepsilon}\\, \\mathcal{L}\\bigl(z_{\\rm test},\\ \\theta_\\varepsilon(z_{\\text{train}})\\bigr) \\Big|_{\\varepsilon=0}$$\n",
    "\n",
    "### 2.1 Décomposition avec la chain rule\n",
    "\n",
    "On peut voir cette expression comme $\\frac{d}{d\\varepsilon} f(g(\\varepsilon))$ avec :\n",
    "- $g(\\varepsilon) = \\theta_\\varepsilon(z_{\\text{train}})$ (les poids après up-weight)\n",
    "- $f(\\theta) = \\mathcal{L}(z_{\\rm test}, \\theta)$ (la loss de test)\n",
    "\n",
    "Par la **chain rule** :\n",
    "\n",
    "$$\\boxed{\\frac{d}{d\\varepsilon}\\, \\mathcal{L}\\bigl(z_{\\rm test},\\ \\theta_\\varepsilon\\bigr) \\Big|_{\\varepsilon=0} = \\nabla_\\theta \\mathcal{L}(z_{\\rm test}, \\hat{\\theta}) \\times \\frac{d\\theta_\\varepsilon}{d\\varepsilon}\\Big|_{\\varepsilon=0}}$$\n",
    "\n",
    "Il nous faut donc calculer $\\frac{d\\theta_\\varepsilon}{d\\varepsilon}\\Big|_{\\varepsilon=0}$.\n",
    "\n",
    "### 2.2 Qu'est-ce que $\\theta_\\varepsilon(z_{\\text{train}})$ ?\n",
    "\n",
    "Pour faire l'\"up-weight\" de $z_{\\text{train}}$ de $\\varepsilon$, on perturbe la loss globale :\n",
    "\n",
    "$$R_\\varepsilon(\\theta, z_{\\text{train}}) = \\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(z_i,\\theta) + \\varepsilon\\,\\mathcal{L}(z_{\\rm train},\\theta)$$\n",
    "\n",
    "Puis on cherche les poids $\\theta_\\varepsilon$ qui **minimisent cette nouvelle loss** :\n",
    "\n",
    "$$\\theta_\\varepsilon(z_{\\text{train}}) = \\arg\\min_{\\theta}\\;R_\\varepsilon(\\theta, z_{\\text{train}})$$\n",
    "\n",
    "Ce qui revient à résoudre (condition d'optimalité du premier ordre) :\n",
    "\n",
    "$$\\nabla_\\theta R_\\varepsilon\\bigl(\\theta_\\varepsilon\\bigr) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Approximation de Taylor à l'ordre 1\n",
    "\n",
    "Pour un petit $\\varepsilon$ (proche de 0), $\\theta_\\varepsilon$ est proche de $\\hat{\\theta}$. On développe avec **Taylor** :\n",
    "\n",
    "$$\\nabla_\\theta R_\\varepsilon\\bigl(\\theta_\\varepsilon\\bigr) \\approx \\nabla_\\theta R_\\varepsilon(\\hat{\\theta}) + \\nabla^2_\\theta R_\\varepsilon(\\hat{\\theta}) \\cdot (\\theta_\\varepsilon - \\hat{\\theta})$$\n",
    "\n",
    "En utilisant $\\nabla_\\theta R_\\varepsilon(\\theta_\\varepsilon) = 0$ et en isolant $(\\theta_\\varepsilon - \\hat{\\theta})$ :\n",
    "\n",
    "$$\\theta_\\varepsilon - \\hat{\\theta} \\approx - \\left[ \\nabla^2_\\theta R_\\varepsilon(\\hat{\\theta}) \\right]^{-1} \\nabla_\\theta R_\\varepsilon(\\hat{\\theta})$$\n",
    "\n",
    "Or, puisque $\\hat{\\theta}$ minimise la loss originale, on a $\\frac{1}{n}\\sum_{i=1}^n \\nabla_\\theta \\mathcal{L}(z_i,\\hat{\\theta}) = 0$.\n",
    "\n",
    "Donc :\n",
    "$$\\nabla_\\theta R_\\varepsilon(\\hat{\\theta}) = \\varepsilon\\,\\nabla_\\theta \\mathcal{L}(z_{\\rm train},\\hat{\\theta})$$\n",
    "\n",
    "Et à l'ordre 1 en $\\varepsilon$ :\n",
    "$$\\nabla^2_\\theta R_\\varepsilon(\\hat{\\theta}) \\approx \\underbrace{\\frac{1}{n}\\sum_{i=1}^n \\nabla^2_\\theta \\mathcal{L}(z_i,\\hat{\\theta})}_{= H_\\theta(\\hat{\\theta})}$$\n",
    "\n",
    "### 2.4 Résultat : formule de $\\frac{d\\theta_\\varepsilon}{d\\varepsilon}$\n",
    "\n",
    "En combinant et en dérivant par rapport à $\\varepsilon$ :\n",
    "\n",
    "$$\\boxed{\\frac{d\\theta_\\varepsilon}{d\\varepsilon}\\Big|_{\\varepsilon=0} = - H_\\theta^{-1}(\\hat{\\theta}) \\cdot \\nabla_\\theta \\mathcal{L}(z_{\\rm train},\\hat{\\theta})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Formule finale de l'influence\n",
    "\n",
    "En multipliant par $\\nabla_\\theta \\mathcal{L}(z_{\\rm test}, \\hat{\\theta})$ (chain rule de l'étape 2.1) :\n",
    "\n",
    "$$\\boxed{\\mathrm{Influence}(z_{\\rm train} \\to z_{\\rm test}) = -\\nabla_\\theta \\mathcal{L}(z_{\\rm test}, \\hat{\\theta})^T \\cdot H_\\theta^{-1}(\\hat{\\theta}) \\cdot \\nabla_\\theta \\mathcal{L}(z_{\\rm train}, \\hat{\\theta})}$$\n",
    "\n",
    "### Structure du calcul\n",
    "\n",
    "```\n",
    "1. grad_test = nabla_theta L(z_test)        # Gradient sur le test\n",
    "2. ihvp = H^{-1} @ grad_test                # IHVP (Inverse Hessian Vector Product)\n",
    "3. Pour chaque z_train :\n",
    "   a. grad_train = nabla_theta L(z_train)   # Gradient sur le train\n",
    "   b. influence = -ihvp^T @ grad_train      # Produit scalaire\n",
    "```\n",
    "\n",
    "**Le défi principal** : calculer $H^{-1} \\cdot v$ efficacement (IHVP) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Pourquoi Fisher au lieu de la Hessienne ? <a name=\"3.-Pourquoi-Fisher-?\"></a>\n",
    "\n",
    "### 3.1 Problème avec la Hessienne exacte\n",
    "\n",
    "La Hessienne exacte nécessite une **double backpropagation** très coûteuse :\n",
    "\n",
    "$$H_\\theta = \\nabla^2_\\theta \\mathcal{L} = \\underbrace{J_{y\\theta}^T \\, H_y \\, J_{y\\theta}}_{\\text{Gauss-Newton (facile)}} + \\underbrace{\\sum_i \\frac{\\partial \\mathcal{L}}{\\partial y_i} \\nabla^2_\\theta y_i}_{\\text{Cauchemar computationnel}}$$\n",
    "\n",
    "Où :\n",
    "- $J_{y\\theta} = \\frac{\\partial y}{\\partial \\theta}$ : Jacobienne des sorties du modèle\n",
    "- $H_y = \\frac{\\partial^2 \\mathcal{L}}{\\partial y^2}$ : Hessienne de la loss par rapport aux sorties\n",
    "- Le dernier terme nécessite les dérivées secondes à travers tout le réseau !\n",
    "\n",
    "### 3.2 Approximation Gauss-Newton\n",
    "\n",
    "On ignore le terme coûteux et on garde :\n",
    "\n",
    "$$H_{\\theta} \\approx G_\\theta = J_{y\\theta}^T \\, H_y \\, J_{y\\theta}$$\n",
    "\n",
    "C'est la **Gauss-Newton Hessian (GNH)**, aussi appelée **matrice de Fisher empirique**.\n",
    "\n",
    "### 3.3 Lien avec les gradients\n",
    "\n",
    "Par la chain rule, le gradient de la loss est :\n",
    "$$g = \\nabla_\\theta \\mathcal{L} = J_{y\\theta}^T \\cdot \\nabla_y \\mathcal{L}$$\n",
    "\n",
    "Donc :\n",
    "$$g \\, g^T = J_{y\\theta}^T \\, (\\nabla_y \\mathcal{L})(\\nabla_y \\mathcal{L})^T \\, J_{y\\theta}$$\n",
    "\n",
    "Pour des losses de type cross-entropy, on a $\\mathbb{E}[(\\nabla_y \\mathcal{L})(\\nabla_y \\mathcal{L})^T] \\approx H_y$.\n",
    "\n",
    "D'où **l'approximation Fisher** :\n",
    "\n",
    "$$\\boxed{H_\\theta \\approx F = \\mathbb{E}[g \\, g^T] = \\frac{1}{n}\\sum_{i=1}^n \\nabla_\\theta \\mathcal{L}(z_i) \\cdot \\nabla_\\theta \\mathcal{L}(z_i)^T}$$\n",
    "\n",
    "### 3.4 Avantages de Fisher\n",
    "\n",
    "| Aspect | Hessienne exacte | Fisher |\n",
    "|--------|------------------|--------|\n",
    "| **Calcul** | Double backprop | Produit externe de gradients |\n",
    "| **Définie positive** | Pas toujours | Toujours ! |\n",
    "| **Inversibilité** | Problématique | Avec damping, OK |\n",
    "\n",
    "### 3.5 Damping\n",
    "\n",
    "Comme les réseaux ne sont pas convexes, on ajoute un terme de **damping** $\\lambda$ :\n",
    "\n",
    "$$H^{-1} \\approx (G + \\lambda I)^{-1}$$\n",
    "\n",
    "Cela garantit l'inversibilité et stabilise le calcul."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. KFAC : Factorisation de Kronecker par couche <a name=\"4.-KFAC\"></a>\n",
    "\n",
    "### 4.1 Le problème de mémoire\n",
    "\n",
    "Même la matrice de Fisher est énorme :\n",
    "- Pour $p$ paramètres : $F$ a taille $p \\times p$\n",
    "- Un réseau de 1M paramètres : $F$ = 4 TB de mémoire !\n",
    "\n",
    "### 4.2 Idée clé : structure bloc-diagonale\n",
    "\n",
    "KFAC exploite la structure par couches :\n",
    "\n",
    "$$G \\approx \\begin{bmatrix} G_{1,1} & 0 & \\cdots \\\\ 0 & G_{2,2} & \\\\ \\vdots & & \\ddots \\end{bmatrix}$$\n",
    "\n",
    "On ignore les interactions **entre** couches et on ne garde que les blocs **diagonaux** $G_{l,l}$ pour chaque couche $l$.\n",
    "\n",
    "### 4.3 Factorisation de Kronecker pour une couche Dense\n",
    "\n",
    "Pour une couche Dense $h = Wa + b$ :\n",
    "- $a$ : activation d'**entrée** (dimension $d_{in}$)\n",
    "- $h$ : pré-activation de **sortie** (dimension $d_{out}$)\n",
    "- $\\delta = \\frac{\\partial \\mathcal{L}}{\\partial h}$ : gradient par rapport à la sortie\n",
    "\n",
    "Le gradient par rapport aux poids est un **produit externe** :\n",
    "$$\\nabla_W \\mathcal{L} = a \\cdot \\delta^T$$\n",
    "\n",
    "En vectorisant : $\\text{vec}(\\nabla_W \\mathcal{L}) = \\delta \\otimes a$\n",
    "\n",
    "**Approximation KFAC** :\n",
    "$$F_W \\approx \\underbrace{\\mathbb{E}[\\delta \\delta^T]}_{G \\;(d_{out} \\times d_{out})} \\;\\otimes\\; \\underbrace{\\mathbb{E}[aa^T]}_{A \\;(d_{in} \\times d_{in})}$$\n",
    "\n",
    "### 4.4 Réduction de mémoire\n",
    "\n",
    "Au lieu de stocker $F$ de taille $(d_{in} \\cdot d_{out})^2$, on stocke :\n",
    "- $A$ : $d_{in}^2$ éléments\n",
    "- $G$ : $d_{out}^2$ éléments\n",
    "\n",
    "**Exemple** : couche 1000 $\\to$ 1000\n",
    "- Fisher complète : $10^{12}$ éléments\n",
    "- KFAC : $2 \\times 10^6$ éléments (**500 000x** moins !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Inversion avec le produit de Kronecker\n",
    "\n",
    "Grâce à la propriété : $(G \\otimes A)^{-1} = G^{-1} \\otimes A^{-1}$\n",
    "\n",
    "On inverse deux **petites** matrices au lieu d'une grande !\n",
    "\n",
    "### 4.6 Pi-damping\n",
    "\n",
    "Le damping standard $(G \\otimes A + \\lambda I)$ n'est pas factorisable.\n",
    "\n",
    "KFAC utilise le **$\\pi$-damping** :\n",
    "\n",
    "$$\\pi = \\sqrt{\\frac{\\text{tr}(A) / \\dim(A)}{\\text{tr}(G) / \\dim(G)}}$$\n",
    "\n",
    "$$G \\otimes A + \\lambda I \\approx \\left(G + \\frac{\\sqrt{\\lambda}}{\\pi} I\\right) \\otimes \\left(A + \\sqrt{\\lambda} \\cdot \\pi \\cdot I\\right)$$\n",
    "\n",
    "Le rôle de $\\pi$ est d'**équilibrer** le damping entre $A$ et $G$ selon leurs échelles spectrales.\n",
    "\n",
    "### 4.7 Résolution du système KFAC\n",
    "\n",
    "Pour résoudre $(G \\otimes A + \\lambda I) \\cdot x = v$ :\n",
    "\n",
    "1. Reshape $v$ en matrice $V$ de taille $(d_{in}, d_{out})$\n",
    "2. $A_{reg} = A + \\sqrt{\\lambda} \\cdot \\pi \\cdot I$, $G_{reg} = G + \\frac{\\sqrt{\\lambda}}{\\pi} \\cdot I$\n",
    "3. Résoudre $G_{reg} \\cdot X = V^T$ pour $X$\n",
    "4. Résoudre $A_{reg} \\cdot Y = X^T$ pour $Y$\n",
    "5. La solution est $\\text{vec}(Y)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démonstration du produit de Kronecker\n",
    "def demo_kronecker_product():\n",
    "    \"\"\"\n",
    "    Illustre la factorisation de Kronecker utilisée par KFAC.\n",
    "    \"\"\"\n",
    "    # Petites matrices pour démonstration\n",
    "    A = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "    G = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "    \n",
    "    # Produit de Kronecker\n",
    "    K = np.kron(G, A)\n",
    "    print(\"Produit de Kronecker: G x A\")\n",
    "    print(f\"A shape: {A.shape}, G shape: {G.shape}\")\n",
    "    print(f\"G x A shape: {K.shape}\")\n",
    "    print()\n",
    "    \n",
    "    # Vérification: (G x A)^{-1} = G^{-1} x A^{-1}\n",
    "    K_inv_direct = np.linalg.inv(K)\n",
    "    K_inv_kron = np.kron(np.linalg.inv(G), np.linalg.inv(A))\n",
    "    \n",
    "    print(\"Verification: (G x A)^{-1} = G^{-1} x A^{-1}\")\n",
    "    print(f\"Erreur max: {np.max(np.abs(K_inv_direct - K_inv_kron)):.2e}\")\n",
    "    \n",
    "demo_kronecker_product()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. EKFAC : Correction des valeurs propres <a name=\"5.-EKFAC\"></a>\n",
    "\n",
    "### 5.1 Limitation de KFAC\n",
    "\n",
    "KFAC suppose que les valeurs propres de $G \\otimes A$ sont les produits des valeurs propres individuelles :\n",
    "\n",
    "$$\\lambda_{ij}^{KFAC} = \\lambda_i^G \\cdot \\lambda_j^A$$\n",
    "\n",
    "C'est une **approximation** qui peut être inexacte.\n",
    "\n",
    "### 5.2 Principe d'EKFAC\n",
    "\n",
    "EKFAC (Eigenvalue-corrected KFAC) calcule les **vraies valeurs propres** dans la base propre de Kronecker.\n",
    "\n",
    "**Étape 1 : Décomposition spectrale**\n",
    "\n",
    "$$A = U_A \\Lambda_A U_A^T, \\quad G = U_G \\Lambda_G U_G^T$$\n",
    "\n",
    "**Étape 2 : Projection dans la base propre**\n",
    "\n",
    "Pour chaque échantillon :\n",
    "- $a_{kfe} = U_A^T \\cdot a$ (activation projetée)\n",
    "- $\\delta_{kfe} = U_G^T \\cdot \\delta$ (gradient projeté)\n",
    "\n",
    "**Étape 3 : Calcul des vraies valeurs propres**\n",
    "\n",
    "$$\\Lambda_{corr}[i,j] = \\frac{1}{n} \\sum_{samples} (\\delta_{kfe}[i])^2 \\cdot (a_{kfe}[j])^2$$\n",
    "\n",
    "### 5.3 Résolution EKFAC\n",
    "\n",
    "Dans la base propre, le système est **diagonal** :\n",
    "\n",
    "$$(\\Lambda_{corr} + \\lambda I) \\cdot x_{kfe} = v_{kfe}$$\n",
    "\n",
    "Solution élémentaire :\n",
    "$$x_{kfe}[i,j] = \\frac{v_{kfe}[i,j]}{\\Lambda_{corr}[i,j] + \\lambda}$$\n",
    "\n",
    "Puis transformation inverse : $x = (U_A \\otimes U_G) \\cdot x_{kfe}$\n",
    "\n",
    "### 5.4 Comparaison KFAC vs EKFAC\n",
    "\n",
    "| Aspect | KFAC | EKFAC |\n",
    "|--------|------|-------|\n",
    "| **Valeurs propres** | $\\lambda^G_i \\cdot \\lambda^A_j$ (approximées) | Empiriques (exactes) |\n",
    "| **Coût** | 1 passe sur les données | 2 passes (facteurs + eigenvalues) |\n",
    "| **Précision** | Bonne | Meilleure |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Implémentation dans Influenciae <a name=\"6.-Implémentation\"></a>\n",
    "\n",
    "### 6.1 La classe `InfluenceModel`\n",
    "\n",
    "Influenciae utilise un wrapper `InfluenceModel` pour sélectionner les poids à surveiller :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation d'un modele exemple\n",
    "def create_example_model():\n",
    "    \"\"\"Cree un MLP simple pour demonstration.\"\"\"\n",
    "    inputs = layers.Input(shape=(100,), name=\"input\")\n",
    "    \n",
    "    # Couche \"feature extractor\" (on pourrait vouloir l'ignorer)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"feature_extractor\")(inputs)\n",
    "    \n",
    "    # Couches de classification (on veut surveiller celles-ci)\n",
    "    x = layers.Dense(32, activation=\"relu\", name=\"hidden\")(x)\n",
    "    outputs = layers.Dense(10, activation=None, name=\"classifier\")(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=outputs, name=\"example_mlp\")\n",
    "\n",
    "model = create_example_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function SANS reduction (obligatoire pour influenciae)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True, reduction=Reduction.NONE)\n",
    "\n",
    "def process_batch_for_loss(batch):\n",
    "    \"\"\"Convertit un batch en format (inputs, labels, sample_weights).\"\"\"\n",
    "    inputs, labels = batch\n",
    "    sample_weights = tf.ones_like(labels, dtype=tf.float32)\n",
    "    return inputs, labels, sample_weights\n",
    "\n",
    "# Surveiller SEULEMENT les couches de classification (hidden + classifier)\n",
    "influence_model = InfluenceModel(\n",
    "    model,\n",
    "    start_layer=\"hidden\",    # Par nom de couche\n",
    "    last_layer=\"classifier\",\n",
    "    loss_function=loss_fn,\n",
    "    process_batch_for_loss_fn=process_batch_for_loss\n",
    ")\n",
    "\n",
    "print(f\"Nombre de parametres surveilles: {influence_model.nb_params}\")\n",
    "print(f\"Poids surveilles: {[w.name for w in influence_model.weights]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 `tf.GradientTape(persistent=True)` : l'équivalent des hooks PyTorch\n",
    "\n",
    "Pour KFAC/EKFAC, on a besoin de :\n",
    "1. **Activations d'entrée** $a$ de chaque couche\n",
    "2. **Gradients de sortie** $\\delta$ de chaque couche\n",
    "\n",
    "En PyTorch, on utilise des **hooks**. En TensorFlow, on utilise `GradientTape(persistent=True)` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration de la capture des gradients\n",
    "x_sample = tf.random.normal((1, 100))\n",
    "y_sample = tf.constant([3])\n",
    "\n",
    "classifier_layer = model.get_layer(\"classifier\")\n",
    "\n",
    "# 1. Creer un sous-modele pour obtenir l'activation d'entree de la couche\n",
    "layer_input_model = tf.keras.Model(\n",
    "    inputs=model.input,\n",
    "    outputs=classifier_layer.input\n",
    ")\n",
    "\n",
    "# 2. Utiliser persistent=True pour pouvoir calculer plusieurs gradients\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    output = model(x_sample)\n",
    "    loss = loss_fn(y_sample, output)\n",
    "    loss_scalar = tf.reduce_sum(loss)\n",
    "\n",
    "# 3. Recuperer a (activation d'entree de la couche)\n",
    "a = layer_input_model(x_sample)  # Shape: (1, in_features)\n",
    "a = tf.squeeze(a, axis=0)  # Shape: (in_features,)\n",
    "\n",
    "# 4. Recuperer delta = dL/dh via le gradient du bias\n",
    "#    Pour Dense: y = Wa + b, donc dL/db = dL/dy = delta\n",
    "delta = tape.gradient(loss_scalar, classifier_layer.bias)\n",
    "\n",
    "# 5. On peut aussi obtenir le gradient du kernel\n",
    "grad_kernel = tape.gradient(loss_scalar, classifier_layer.kernel)\n",
    "\n",
    "del tape\n",
    "\n",
    "print(f\"Activation d'entree (a): shape={a.shape}\")\n",
    "print(f\"Gradient de sortie (delta = dL/dy): shape={delta.shape}\")\n",
    "print(f\"Gradient du kernel (dL/dW): shape={grad_kernel.shape}\")\n",
    "print(f\"\\nVerification: dL/dW = a x delta (produit externe)\")\n",
    "print(f\"  grad_W ~= outer(a, delta): {np.allclose(grad_kernel.numpy(), np.outer(a, delta), rtol=0.1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Les calculateurs IHVP dans Influenciae\n",
    "\n",
    "| Classe | Méthode | Complexité mémoire |\n",
    "|--------|---------|-------------------|\n",
    "| `ExactIHVP` | Hessienne exacte | $O(p^2)$ |\n",
    "| `FisherIHVP` | $F = \\mathbb{E}[gg^T]$ | $O(p^2)$ |\n",
    "| `KFACIHVP` | $F \\approx G \\otimes A$ | $O(\\sum d_{in}^2 + d_{out}^2)$ |\n",
    "| `EKFACIHVP` | KFAC + eigenvalues corrigées | $O(\\sum d_{in}^2 + d_{out}^2)$ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creer des donnees pour la demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "X_train = np.random.randn(n_samples, 100).astype(np.float32)\n",
    "y_train = np.random.randint(0, 10, n_samples).astype(np.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
    "\n",
    "# Creer EKFACIHVP\n",
    "print(\"Creation de EKFACIHVP...\")\n",
    "ekfac_ihvp = EKFACIHVP(\n",
    "    model=influence_model,\n",
    "    train_dataset=train_dataset,\n",
    "    damping=1e-3,\n",
    "    update_eigen=True  # Calculer les vraies valeurs propres\n",
    ")\n",
    "\n",
    "print(f\"\\nBlocs KFAC calcules!\")\n",
    "for layer_idx, (A, G) in ekfac_ihvp.kfac_blocks.items():\n",
    "    layer_info = ekfac_ihvp.layer_info[layer_idx]\n",
    "    print(f\"\\nLayer {layer_idx}: {layer_info['layer'].name}\")\n",
    "    print(f\"  A (covariance activation): shape={A.shape}\")\n",
    "    print(f\"  G (covariance gradient):   shape={G.shape}\")\n",
    "    print(f\"  Eigenvalues (EKFAC):       shape={ekfac_ihvp.evals[layer_idx].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Exemple pratique complet <a name=\"7.-Exemple-pratique\"></a>\n",
    "\n",
    "Mettons tout ensemble pour calculer l'influence :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deel.influenciae.influence import FirstOrderInfluenceCalculator\n",
    "from deel.influenciae.utils import ORDER\n",
    "\n",
    "# 1. Preparer les donnees\n",
    "np.random.seed(42)\n",
    "n_train, n_test = 100, 10\n",
    "\n",
    "X_train = np.random.randn(n_train, 100).astype(np.float32)\n",
    "y_train = np.random.randint(0, 10, n_train).astype(np.int32)\n",
    "\n",
    "X_test = np.random.randn(n_test, 100).astype(np.float32)\n",
    "y_test = np.random.randint(0, 10, n_test).astype(np.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(10)\n",
    "hessian_dataset = tf.data.Dataset.from_tensor_slices((X_train[:50], y_train[:50])).batch(8)\n",
    "\n",
    "print(f\"Train: {n_train} samples, Test: {n_test} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Creer et entrainer un modele\n",
    "model = create_example_model()\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, epochs=5, verbose=0)\n",
    "print(f\"Accuracy: {model.evaluate(X_test, y_test, verbose=0)[1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Wrapper le modele avec InfluenceModel\n",
    "influence_model = InfluenceModel(\n",
    "    model,\n",
    "    start_layer=\"hidden\",\n",
    "    last_layer=\"classifier\",\n",
    "    loss_function=loss_fn,\n",
    "    process_batch_for_loss_fn=process_batch_for_loss\n",
    ")\n",
    "print(f\"Parametres surveilles: {influence_model.nb_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Creer le calculateur IHVP (EKFAC)\n",
    "ihvp_calculator = EKFACIHVP(\n",
    "    model=influence_model,\n",
    "    train_dataset=hessian_dataset,\n",
    "    damping=1e-3,\n",
    "    update_eigen=True\n",
    ")\n",
    "print(\"EKFAC IHVP calculator cree!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Creer le calculateur d'influence\n",
    "influence_calculator = FirstOrderInfluenceCalculator(\n",
    "    influence_model,\n",
    "    train_dataset,\n",
    "    ihvp_calculator\n",
    ")\n",
    "print(\"FirstOrderInfluenceCalculator cree!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Calculer les exemples les plus influents\n",
    "top_k = 5\n",
    "\n",
    "print(f\"\\nRecherche des top-{top_k} exemples influents...\")\n",
    "top_k_results = influence_calculator.top_k(\n",
    "    test_dataset,\n",
    "    train_dataset,\n",
    "    k=top_k,\n",
    "    order=ORDER.DESCENDING\n",
    ")\n",
    "\n",
    "# Afficher les resultats\n",
    "for batch_result in top_k_results:\n",
    "    test_samples, influence_values, train_samples = batch_result\n",
    "    test_inputs, test_labels = test_samples\n",
    "    \n",
    "    for i in range(min(3, len(test_labels))):\n",
    "        print(f\"\\nExemple de test {i} (label={test_labels[i].numpy()}):\")\n",
    "        print(f\"  Top-{top_k} influences: {influence_values[i].numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Resume : Pipeline complet de calcul de l'influence\n",
    "\n",
    "```\n",
    "1. ENTRAINER le modele\n",
    "         |\n",
    "         v\n",
    "2. CALCULER les facteurs KFAC/EKFAC sur le dataset d'entrainement\n",
    "   - Pour chaque couche Dense : extraire A et G\n",
    "   - EKFAC : calculer les valeurs propres corrigees\n",
    "         |\n",
    "         v\n",
    "3. Pour chaque echantillon de TEST z_test :\n",
    "   a) Calculer le gradient : v = nabla_theta L(z_test)\n",
    "   b) Calculer l'IHVP : ihvp = H^{-1} @ v  (via KFAC/EKFAC)\n",
    "         |\n",
    "         v\n",
    "4. Pour chaque echantillon d'ENTRAINEMENT z_train :\n",
    "   a) Calculer le gradient : g = nabla_theta L(z_train)\n",
    "   b) Calculer l'influence : I(z_train, z_test) = -ihvp^T @ g\n",
    "```\n",
    "\n",
    "### Formules cles\n",
    "\n",
    "| Concept | Formule |\n",
    "|---------|--------|\n",
    "| **Influence** | $I(z_{train} \\to z_{test}) = -\\nabla L_{test}^T \\cdot H^{-1} \\cdot \\nabla L_{train}$ |\n",
    "| **Fisher** | $F = \\mathbb{E}[g \\cdot g^T]$ |\n",
    "| **KFAC** | $F \\approx G \\otimes A$ avec $A = \\mathbb{E}[aa^T]$, $G = \\mathbb{E}[\\delta\\delta^T]$ |\n",
    "| **EKFAC** | Comme KFAC mais avec $\\Lambda_{corr}[i,j] = \\mathbb{E}[(\\delta_{kfe}[i])^2 (a_{kfe}[j])^2]$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [Koh & Liang (2017)](https://arxiv.org/abs/1703.04730) - Understanding Black-box Predictions via Influence Functions\n",
    "- [Martens & Grosse (2015)](https://arxiv.org/abs/1503.05671) - Optimizing Neural Networks with Kronecker-factored Approximate Curvature (KFAC)\n",
    "- [George et al. (2018)](https://arxiv.org/abs/1806.03884) - Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis (EKFAC)\n",
    "- [Grosse et al. (2023)](https://arxiv.org/abs/2308.03296) - Studying Large Language Model Generalization with Influence Functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
